{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lacrosse Page Similarity on Wikipedia\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ocamp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import wikipedia as wk\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.manifold import MDS as mds\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial import distance_matrix\n",
    "from bokeh.io import output_file, show\n",
    "from bokeh.plotting import figure, save\n",
    "from bokeh.models import HoverTool, TapTool, OpenURL, Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a list of pages for search of lacrosse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages returned for lacrosse:100\n",
      "\n",
      "Lacrosse\n",
      "Lacrosse (disambiguation)\n",
      "Box lacrosse\n",
      "Buick LaCrosse\n",
      "List of NCAA Division II lacrosse programs\n",
      "Lacrosse ball\n",
      "College lacrosse\n",
      "List of NCAA Division I lacrosse programs\n",
      "Field lacrosse\n",
      "Crystal Mangum\n",
      "National Lacrosse League\n",
      "Murder of Yeardley Love\n",
      "Lacrosse (satellite)\n",
      "Women's lacrosse\n",
      "NCAA Division I Women's Lacrosse Championship\n",
      "NCAA Division I Men's Lacrosse Championship\n",
      "History of lacrosse\n",
      "Duke lacrosse case\n",
      "Myles Jones\n",
      "Major League Lacrosse\n",
      "Lacrosse stick\n",
      "San Diego Seals\n",
      "Johns Hopkins Blue Jays men's lacrosse\n",
      "Jordan Levine\n",
      "Lacrosse in Canada\n",
      "Johns Hopkins Blue Jays\n",
      "Paul Rabil\n",
      "Steven Brooks (lacrosse)\n",
      "List of Major League Lacrosse awards\n",
      "US Lacrosse\n",
      "Miles Thompson\n",
      "Roy Colsey\n",
      "Lacrosse National Hall of Fame and Museum\n",
      "NCAA Men's Lacrosse Championship\n",
      "IMG Academy\n",
      "Brett Queener\n",
      "Randy Mearns\n",
      "Johnny Christmas\n",
      "Iroquois men's national lacrosse team\n",
      "Professional Lacrosse League\n",
      "Joey Cupido\n",
      "Maverik Lacrosse\n",
      "Ian Llord\n",
      "Michigan Wolverines men's lacrosse\n",
      "Blake Miller (lacrosse)\n",
      "Mark Matthews (lacrosse)\n",
      "John Tavares (lacrosse)\n",
      "UMass Minutemen lacrosse\n",
      "David Huntley\n",
      "Canadian Lacrosse Association\n",
      "Warrior Sports\n",
      "Canada men's national lacrosse team\n",
      "Athan Iannucci\n",
      "Dillon Ward\n",
      "Brodie Merrill\n",
      "Greg Bice\n",
      "Denver Pioneers men's lacrosse\n",
      "Alex Smith (lacrosse)\n",
      "Men's Collegiate Lacrosse Association\n",
      "David Morrow (sports)\n",
      "Maryland Terrapins men's lacrosse\n",
      "Kyle Ross\n",
      "Jordan Hall (lacrosse)\n",
      "Bill McGlone\n",
      "John Tucker (lacrosse)\n",
      "Fairfield Stags men's lacrosse\n",
      "Jeremy Thompson (lacrosse)\n",
      "Delaware Fightin' Blue Hens men's lacrosse\n",
      "Lacrosse in Pennsylvania\n",
      "Lyle Thompson\n",
      "STX (sports manufacturer)\n",
      "Paul Cantabene\n",
      "Minnesota Lakers Lacrosse Club\n",
      "Peet Poillon\n",
      "Lacrosse in Spain\n",
      "Syracuse Orange men's lacrosse\n",
      "Lacrosse in Australia\n",
      "Jarett Park\n",
      "Kevin Huntley (lacrosse)\n",
      "Big Ten Conference\n",
      "List of Australian Lacrosse best and fairest players\n",
      "Cindy Timchal\n",
      "Joe Walters\n",
      "Matt Vinc\n",
      "Trevor Tierney\n",
      "Gavin Prout\n",
      "Shawn Evans (lacrosse)\n",
      "Kevin Cooper (lacrosse)\n",
      "List of South Australian Lacrosse Premiers\n",
      "John Grant Jr.\n",
      "John Gagliardi (lacrosse)\n",
      "Nolan Godfrey\n",
      "Loyola Greyhounds men's lacrosse\n",
      "Bryant Bulldogs men's lacrosse\n",
      "Canada\n",
      "Andrew Combs\n",
      "Lacrosse Hall of Fame\n",
      "Chris Hogan (American football)\n",
      "Big East Conference\n",
      "United States men's national lacrosse team\n"
     ]
    }
   ],
   "source": [
    "search_term = 'lacrosse'\n",
    "results_limit = 100\n",
    "\n",
    "page_list = wk.search(search_term, results = results_limit)\n",
    "print('Pages returned for lacrosse:' + str(len(page_list)) + '\\n')\n",
    "for i in page_list: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return content and urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ocamp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\ocamp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure on 'Lacrosse (disambiguation)'\n",
      "Failure on 'Kevin Huntley (lacrosse)'\n",
      "Failure on 'Lacrosse Hall of Fame'\n",
      "\n",
      "Content size: 97\n",
      "\n",
      "URL size: 97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content = []\n",
    "url = []\n",
    "returned_pages = []\n",
    "\n",
    "for i in page_list: \n",
    "    try:\n",
    "        content.append(wk.page(i).content)\n",
    "        url.append(wk.page(i).url)\n",
    "        returned_pages.append(i)\n",
    "    except:\n",
    "        print('Failure on \\'' + i + '\\'')\n",
    "        \n",
    "print('\\n' + 'Content size: ' + str(len(content)) + '\\n')\n",
    "print('URL size: ' + str(len(url)) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_content = []\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "rt = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for i in content: \n",
    "    cleaned_content.append([ps.stem(w) for w in rt.tokenize(i.lower()) if not w in stop_words and w.isalpha()])\n",
    "\n",
    "content2 = []\n",
    "for i in cleaned_content: \n",
    "    content2.append(' '.join(i))\n",
    "    \n",
    "vec = CountVectorizer()\n",
    "content_vec = vec.fit_transform(content2)\n",
    "df = pd.DataFrame(content_vec.toarray(), columns = vec.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Distance Matrix and Scale to 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ocamp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\mds.py:421: UserWarning: The MDS API has changed. ``fit`` now constructs an dissimilarity matrix from data. To use a custom dissimilarity matrix, set ``dissimilarity='precomputed'``.\n",
      "  warnings.warn(\"The MDS API has changed. ``fit`` now constructs an\"\n"
     ]
    }
   ],
   "source": [
    "m = mds(n_components=2,n_init=50,random_state=9,max_iter=500,dissimilarity='euclidean') \n",
    "\n",
    "dist_mat = distance_matrix(df,df)\n",
    "\n",
    "fnl = m.fit_transform(dist_mat)\n",
    "\n",
    "fnl_df = pd.DataFrame(fnl)\n",
    "fnl_df['Page'] = returned_pages\n",
    "fnl_df['Url'] = url\n",
    "fnl_df.columns = ['X','Y','Page','Url']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get addition information for viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "min_len = 1000000000\n",
    "len_list = []\n",
    "\n",
    "for i in content:\n",
    "    x = len(i)\n",
    "    len_list.append(x)\n",
    "    if x > max_len: \n",
    "        max_len = x\n",
    "    if x < min_len:\n",
    "        min_len = x\n",
    "        \n",
    "range = max_len - min_len\n",
    "norm_len = []\n",
    "\n",
    "for i in len_list:\n",
    "    norm_len.append( ( (i-min_len)/range ) * 50 )\n",
    "\n",
    "    \n",
    "fnl_df['n_Article_Length'] = norm_len\n",
    "fnl_df['Article_Length'] = len_list\n",
    "fnl_df.columns = ['X','Y','Page','Url','n_Article_Length','Article_Length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize with bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Projects\\\\Python\\\\Wikipedia_Lacrosse_Pages\\\\Lacrosse_1.html'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viz_title = 'Wikipedia articles for \\'' + search_term + '\\' Hover to see Page. Click to go to the Wikipedia Page.'\n",
    "\n",
    "output_file('Lacrosse_1.html')\n",
    "plot=figure(plot_width=1000, plot_height=1000, tools='tap',  title = viz_title , toolbar_location='below')\n",
    "plot.circle(x='X',y='Y', color = 'black', size= 'n_Article_Length',source=fnl_df)\n",
    "plot.add_tools(HoverTool(\n",
    "    tooltips=[\n",
    "        ('Page: ', '@Page'),\n",
    "        ('URL: ', '@Url'),\n",
    "        ('Article Length: ', '@Article_Length')\n",
    "    ]\n",
    "))\n",
    "w_url = \"@Url\"\n",
    "taptool=plot.select(type=TapTool)\n",
    "taptool.callback=OpenURL(url=w_url)\n",
    "show(plot)\n",
    "save(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-process text for only the beginning of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_content = []\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "rt = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for i in content: \n",
    "    cleaned_content.append([ps.stem(w) for w in rt.tokenize(i.lower()) if not w in stop_words and w.isalpha()])\n",
    "\n",
    "content2 = []\n",
    "for i in cleaned_content: \n",
    "    temp = ' '.join(i)\n",
    "    temp = temp[:2000]\n",
    "    content2.append(temp)\n",
    "    \n",
    "vec = CountVectorizer()\n",
    "content_vec = vec.fit_transform(content2)\n",
    "df = pd.DataFrame(content_vec.toarray(), columns = vec.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-process distance and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ocamp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\mds.py:421: UserWarning: The MDS API has changed. ``fit`` now constructs an dissimilarity matrix from data. To use a custom dissimilarity matrix, set ``dissimilarity='precomputed'``.\n",
      "  warnings.warn(\"The MDS API has changed. ``fit`` now constructs an\"\n"
     ]
    }
   ],
   "source": [
    "m = mds(n_components=2,n_init=50,random_state=9,max_iter=500,dissimilarity='euclidean') \n",
    "\n",
    "dist_mat = distance_matrix(df,df)\n",
    "\n",
    "fnl = m.fit_transform(dist_mat)\n",
    "\n",
    "fnl_df = pd.DataFrame(fnl)\n",
    "fnl_df['Page'] = returned_pages\n",
    "fnl_df['Url'] = url\n",
    "fnl_df.columns = ['X','Y','Page','Url']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-process additional info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "min_len = 1000000000\n",
    "len_list = []\n",
    "\n",
    "for i in content:\n",
    "    x = len(i)\n",
    "    len_list.append(x)\n",
    "    if x > max_len: \n",
    "        max_len = x\n",
    "    if x < min_len:\n",
    "        min_len = x\n",
    "        \n",
    "range = max_len - min_len\n",
    "norm_len = []\n",
    "\n",
    "for i in len_list:\n",
    "    norm_len.append( ( (i-min_len)/range ) * 50 )\n",
    "\n",
    "    \n",
    "fnl_df['n_Article_Length'] = norm_len\n",
    "fnl_df['Article_Length'] = len_list\n",
    "fnl_df.columns = ['X','Y','Page','Url','n_Article_Length','Article_Length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Projects\\\\Python\\\\Wikipedia_Lacrosse_Pages\\\\Lacrosse_2.html'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viz_title = 'Wikipedia articles for \\'' + search_term + '\\' Hover to see Page. Click to go to the Wikipedia Page.'\n",
    "\n",
    "output_file('Lacrosse_2.html')\n",
    "plot=figure(plot_width=1000, plot_height=1000, tools='tap',  title = viz_title , toolbar_location='below')\n",
    "plot.circle(x='X',y='Y', color = 'black', size= 'n_Article_Length',source=fnl_df)\n",
    "plot.add_tools(HoverTool(\n",
    "    tooltips=[\n",
    "        ('Page: ', '@Page'),\n",
    "        ('URL: ', '@Url'),\n",
    "        ('Article Length: ', '@Article_Length')\n",
    "    ]\n",
    "))\n",
    "w_url = \"@Url\"\n",
    "taptool=plot.select(type=TapTool)\n",
    "taptool.callback=OpenURL(url=w_url)\n",
    "show(plot)\n",
    "save(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results look much better. Page content length was significantly impacting distance metrics. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
