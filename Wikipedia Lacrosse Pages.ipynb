{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lacrosse Page Similarity on Wikipedia\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ocamp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import wikipedia as wk\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.manifold import MDS as mds\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial import distance_matrix\n",
    "from bokeh.io import output_file, show\n",
    "from bokeh.plotting import figure, save\n",
    "from bokeh.models import HoverTool, TapTool, OpenURL, Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a list of pages for search of lacrosse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages returned for lacrosse:100\n",
      "\n",
      "Lacrosse\n",
      "Lacrosse (disambiguation)\n",
      "Box lacrosse\n",
      "Buick LaCrosse\n",
      "College lacrosse\n",
      "List of NCAA Division II lacrosse programs\n",
      "Lacrosse ball\n",
      "Field lacrosse\n",
      "Murder of Yeardley Love\n",
      "List of NCAA Division I lacrosse programs\n",
      "Crystal Mangum\n",
      "Lacrosse (satellite)\n",
      "National Lacrosse League\n",
      "Women's lacrosse\n",
      "NCAA Division I Women's Lacrosse Championship\n",
      "Duke lacrosse case\n",
      "NCAA Division I Men's Lacrosse Championship\n",
      "Major League Lacrosse\n",
      "History of lacrosse\n",
      "Lacrosse stick\n",
      "San Diego Seals\n",
      "Myles Jones\n",
      "Jordan Levine\n",
      "Johns Hopkins Blue Jays men's lacrosse\n",
      "Johns Hopkins Blue Jays\n",
      "Steven Brooks (lacrosse)\n",
      "Lacrosse in Canada\n",
      "Paul Rabil\n",
      "List of Major League Lacrosse awards\n",
      "Joey Cupido\n",
      "UMass Minutemen lacrosse\n",
      "Miles Thompson\n",
      "Iroquois men's national lacrosse team\n",
      "NCAA Men's Lacrosse Championship\n",
      "Professional Lacrosse League\n",
      "Lacrosse National Hall of Fame and Museum\n",
      "Dillon Ward\n",
      "IMG Academy\n",
      "Maverik Lacrosse\n",
      "David Morrow (sports)\n",
      "Blake Miller (lacrosse)\n",
      "Mark Matthews (lacrosse)\n",
      "Randy Mearns\n",
      "David Huntley\n",
      "John Tavares (lacrosse)\n",
      "US Lacrosse\n",
      "Warrior Sports\n",
      "Kevin Cooper (lacrosse)\n",
      "Michigan Wolverines men's lacrosse\n",
      "Ian Llord\n",
      "Johnny Christmas\n",
      "Canadian Lacrosse Association\n",
      "Philadelphia Wings (1987â€“2014)\n",
      "Alex Smith (lacrosse)\n",
      "Jordan Hall (lacrosse)\n",
      "Brodie Merrill\n",
      "Roy Colsey\n",
      "List of Australian Lacrosse best and fairest players\n",
      "John Tucker (lacrosse)\n",
      "Canada men's national lacrosse team\n",
      "Athan Iannucci\n",
      "Shawn Evans (lacrosse)\n",
      "Syracuse Orange men's lacrosse\n",
      "Jeremy Thompson (lacrosse)\n",
      "Greg Bice\n",
      "Delaware Fightin' Blue Hens men's lacrosse\n",
      "Brett Queener\n",
      "Minnesota Lakers Lacrosse Club\n",
      "List of South Australian Lacrosse Premiers\n",
      "Kyle Ross\n",
      "List of national lacrosse governing bodies\n",
      "STX (sports manufacturer)\n",
      "Fairfield Stags men's lacrosse\n",
      "Denver Pioneers men's lacrosse\n",
      "Bill McGlone\n",
      "Maryland Terrapins men's lacrosse\n",
      "Paul Cantabene\n",
      "Jean-Baptiste Raymond de Lacrosse\n",
      "Jarett Park\n",
      "Matt Vinc\n",
      "Lyle Thompson\n",
      "Big Ten Conference\n",
      "Morgan State Bears lacrosse\n",
      "Peter Dante\n",
      "Jeff Zywicki\n",
      "Zack Greer\n",
      "John Grant Jr.\n",
      "Mitch Belisle\n",
      "Joe Walters\n",
      "Lacrosse in Spain\n",
      "Men's Collegiate Lacrosse Association\n",
      "Lacrosse in Australia\n",
      "Sean Greenhalgh\n",
      "Canada\n",
      "Kevin Huntley (lacrosse)\n",
      "Chris Hogan (American football)\n",
      "Bryant Bulldogs men's lacrosse\n",
      "Trevor Tierney\n",
      "2015 Women's Lacrosse European Championship\n",
      "Andrew Combs\n"
     ]
    }
   ],
   "source": [
    "search_term = 'lacrosse'\n",
    "results_limit = 100\n",
    "\n",
    "page_list = wk.search(search_term, results = results_limit)\n",
    "print('Pages returned for lacrosse:' + str(len(page_list)) + '\\n')\n",
    "for i in page_list: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return content and urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ocamp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\ocamp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure on 'Lacrosse (disambiguation)'\n",
      "Failure on 'Kevin Huntley (lacrosse)'\n",
      "\n",
      "Content size: 98\n",
      "\n",
      "URL size: 98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content = []\n",
    "url = []\n",
    "returned_pages = []\n",
    "\n",
    "for i in page_list: \n",
    "    try:\n",
    "        content.append(wk.page(i).content)\n",
    "        url.append(wk.page(i).url)\n",
    "        returned_pages.append(i)\n",
    "    except:\n",
    "        print('Failure on \\'' + i + '\\'')\n",
    "        \n",
    "print('\\n' + 'Content size: ' + str(len(content)) + '\\n')\n",
    "print('URL size: ' + str(len(url)) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_content = []\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "rt = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for i in content: \n",
    "    cleaned_content.append([ps.stem(w) for w in rt.tokenize(i.lower()) if not w in stop_words and w.isalpha()])\n",
    "\n",
    "content2 = []\n",
    "for i in cleaned_content: \n",
    "    content2.append(' '.join(i))\n",
    "    \n",
    "vec = CountVectorizer()\n",
    "content_vec = vec.fit_transform(content2)\n",
    "df = pd.DataFrame(content_vec.toarray(), columns = vec.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Distance Matrix and Scale to 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = mds(n_components=2,dissimilarity='precomputed') \n",
    "\n",
    "dist_mat = distance_matrix(df,df)\n",
    "\n",
    "fnl = m.fit_transform(dist_mat)\n",
    "\n",
    "fnl_df = pd.DataFrame(fnl)\n",
    "fnl_df['Page'] = returned_pages\n",
    "fnl_df['Url'] = url\n",
    "fnl_df.columns = ['X','Y','Page','Url']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get addition information for viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "min_len = 1000000000\n",
    "len_list = []\n",
    "\n",
    "for i in content:\n",
    "    x = len(i)\n",
    "    len_list.append(x)\n",
    "    if x > max_len: \n",
    "        max_len = x\n",
    "    if x < min_len:\n",
    "        min_len = x\n",
    "        \n",
    "range = max_len - min_len\n",
    "norm_len = []\n",
    "\n",
    "for i in len_list:\n",
    "    norm_len.append( ( (i-min_len)/range ) * 50 )\n",
    "\n",
    "    \n",
    "fnl_df['n_Article_Length'] = norm_len\n",
    "fnl_df['Article_Length'] = len_list\n",
    "fnl_df.columns = ['X','Y','Page','Url','n_Article_Length','Article_Length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize with bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Projects\\\\Python\\\\Wikipedia_Lacrosse_Pages\\\\Lacrosse.html'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viz_title = 'Wikipedia articles for \\'' + search_term + '\\' Hover to see Page. Click to go to the Wikipedia Page.'\n",
    "\n",
    "output_file('Lacrosse.html')\n",
    "plot=figure(plot_width=1000, plot_height=1000, tools='tap',  title = viz_title , toolbar_location='below')\n",
    "plot.circle(x='X',y='Y', color = 'black', size= 'n_Article_Length',source=fnl_df)\n",
    "plot.add_tools(HoverTool(\n",
    "    tooltips=[\n",
    "        ('Page: ', '@Page'),\n",
    "        ('URL: ', '@Url'),\n",
    "        ('Article Length: ', '@Article_Length')\n",
    "    ]\n",
    "))\n",
    "w_url = \"@Url\"\n",
    "taptool=plot.select(type=TapTool)\n",
    "taptool.callback=OpenURL(url=w_url)\n",
    "show(plot)\n",
    "save(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-process text for only the beginning of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_content = []\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "rt = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for i in content: \n",
    "    cleaned_content.append([ps.stem(w) for w in rt.tokenize(i.lower()) if not w in stop_words and w.isalpha()])\n",
    "\n",
    "content2 = []\n",
    "for i in cleaned_content: \n",
    "    temp = ' '.join(i)\n",
    "    temp = temp[:2000]\n",
    "    content2.append(temp)\n",
    "    \n",
    "vec = CountVectorizer()\n",
    "content_vec = vec.fit_transform(content2)\n",
    "df = pd.DataFrame(content_vec.toarray(), columns = vec.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-process distance and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = mds(n_components=2,dissimilarity='precomputed') \n",
    "\n",
    "dist_mat = distance_matrix(df,df)\n",
    "\n",
    "fnl = m.fit_transform(dist_mat)\n",
    "\n",
    "fnl_df = pd.DataFrame(fnl)\n",
    "fnl_df['Page'] = returned_pages\n",
    "fnl_df['Url'] = url\n",
    "fnl_df.columns = ['X','Y','Page','Url']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-process additional info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "min_len = 1000000000\n",
    "len_list = []\n",
    "\n",
    "for i in content:\n",
    "    x = len(i)\n",
    "    len_list.append(x)\n",
    "    if x > max_len: \n",
    "        max_len = x\n",
    "    if x < min_len:\n",
    "        min_len = x\n",
    "        \n",
    "range = max_len - min_len\n",
    "norm_len = []\n",
    "\n",
    "for i in len_list:\n",
    "    norm_len.append( ( (i-min_len)/range ) * 50 )\n",
    "\n",
    "    \n",
    "fnl_df['n_Article_Length'] = norm_len\n",
    "fnl_df['Article_Length'] = len_list\n",
    "fnl_df.columns = ['X','Y','Page','Url','n_Article_Length','Article_Length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_title = 'Wikipedia articles for \\'' + search_term + '\\' Hover to see Page. Click to go to the Wikipedia Page.'\n",
    "\n",
    "output_file('Lacrosse.html')\n",
    "plot=figure(plot_width=1000, plot_height=1000, tools='tap',  title = viz_title , toolbar_location='below')\n",
    "plot.circle(x='X',y='Y', color = 'black', size= 'n_Article_Length',source=fnl_df)\n",
    "plot.add_tools(HoverTool(\n",
    "    tooltips=[\n",
    "        ('Page: ', '@Page'),\n",
    "        ('URL: ', '@Url'),\n",
    "        ('Article Length: ', '@Article_Length')\n",
    "    ]\n",
    "))\n",
    "w_url = \"@Url\"\n",
    "taptool=plot.select(type=TapTool)\n",
    "taptool.callback=OpenURL(url=w_url)\n",
    "show(plot)\n",
    "#save(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results look much better. Page content length was significantly impacting distance metrics. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
