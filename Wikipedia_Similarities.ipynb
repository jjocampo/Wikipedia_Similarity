{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ocamp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import wikipedia as wk\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.manifold import MDS as mds\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial import distance_matrix\n",
    "from bokeh.io import output_file, show\n",
    "from bokeh.plotting import figure, save\n",
    "from bokeh.models import HoverTool, TapTool, OpenURL, LabelSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wik_sims(term,limit): \n",
    "    page_list = wk.search(term, results = limit)\n",
    "    content = []\n",
    "    url = []\n",
    "    returned_pages = []\n",
    "    for i in page_list: \n",
    "        try:\n",
    "            content.append(wk.page(i).content)\n",
    "            url.append(wk.page(i).url)\n",
    "            returned_pages.append(i)\n",
    "        except:\n",
    "            print('Failure on \\'' + i + '\\'')\n",
    "    cleaned_content = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    rt = RegexpTokenizer(r'\\w+')\n",
    "    ps = PorterStemmer()\n",
    "    for i in content: \n",
    "        cleaned_content.append([ps.stem(w) for w in rt.tokenize(i.lower()) if not w in stop_words and w.isalpha()])\n",
    "    \n",
    "    \n",
    "    content2 = []\n",
    "    for i in cleaned_content: \n",
    "        content2.append(' '.join(i))\n",
    "    \n",
    "    \n",
    "    vec = CountVectorizer()\n",
    "    content_vec = vec.fit_transform(content2)\n",
    "    df = pd.DataFrame(content_vec.toarray(), columns = vec.get_feature_names())\n",
    "    m = mds(n_components=2,n_init=10,random_state=1,dissimilarity='precomputed') \n",
    "    dist_mat = distance_matrix(df,df)\n",
    "    fnl = m.fit_transform(dist_mat)\n",
    "    fnl_df = pd.DataFrame(fnl)\n",
    "    fnl_df['Page'] = returned_pages\n",
    "    fnl_df['Url'] = url\n",
    "    fnl_df.columns = ['X','Y','Page','Url']\n",
    "    max_len = 0\n",
    "    min_len = 1000000000\n",
    "    len_list = []\n",
    "    for i in content:\n",
    "        x = len(i)\n",
    "        len_list.append(x)\n",
    "        if x > max_len: \n",
    "            max_len = x\n",
    "        if x < min_len:\n",
    "            min_len = x\n",
    "    range = max_len - min_len\n",
    "    norm_len = []\n",
    "    for i in len_list:\n",
    "        norm_len.append( ( (i-min_len)/range ) * 50 )\n",
    "    fnl_df['n_Article_Length'] = norm_len\n",
    "    fnl_df['Article_Length'] = len_list\n",
    "    fnl_df.columns = ['X','Y','Page','Url','n_Article_Length','Article_Length']\n",
    "    viz_title = 'Wikipedia articles for \\'' + term + '\\' Hover to see Page. Click to go to the Wikipedia Page.'\n",
    "    output_file(term + '.html')\n",
    "    plot=figure(plot_width=1000, plot_height=1000, tools='tap',  title = viz_title , toolbar_location='below')\n",
    "    plot.circle(x='X',y='Y', color = 'black', size= 'n_Article_Length',source=fnl_df)\n",
    "    plot.add_tools(HoverTool(\n",
    "        tooltips=[\n",
    "            ('Page: ', '@Page'),\n",
    "            ('URL: ', '@Url'),\n",
    "            ('Article Length: ', '@Article_Length')\n",
    "        ]\n",
    "    ))\n",
    "    w_url = \"@Url\"\n",
    "    taptool=plot.select(type=TapTool)\n",
    "    taptool.callback=OpenURL(url=w_url)\n",
    "    #show(plot)\n",
    "    save(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ocamp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\ocamp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure on 'Political voluntarism'\n",
      "Failure on 'Political officer'\n",
      "Failure on 'Crypto-politics'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ocamp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\ocamp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure on 'Sports kit'\n",
      "Failure on 'Sports City'\n",
      "Failure on 'Titan Sports'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ocamp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\ocamp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure on 'Cable car'\n",
      "Failure on 'Passenger car'\n",
      "Failure on 'Armored car'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ocamp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\ocamp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure on 'Musics'\n",
      "Failure on 'Jam music'\n",
      "Failure on 'Oriental music'\n",
      "Failure on 'Fire Music'\n",
      "Failure on 'British music'\n",
      "Failure on 'Roots music'\n",
      "Failure on 'Battle Music'\n",
      "Failure on 'Music player'\n",
      "Failure on 'Boss Music'\n",
      "Failure on 'Swamp music'\n",
      "Failure on 'Garage'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ocamp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\ocamp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure on 'College Girl'\n",
      "Failure on 'Jordan College'\n",
      "Failure on 'Delhi College'\n",
      "Failure on 'Centenary College'\n",
      "Failure on 'Delta College'\n",
      "Failure on 'Presidency College'\n",
      "Failure on 'Hill college'\n",
      "Failure on 'Bethel College'\n",
      "Failure on 'College Park'\n",
      "Failure on 'Private college'\n",
      "Failure on 'St Aidan's College'\n",
      "Failure on 'Gordon College'\n"
     ]
    }
   ],
   "source": [
    "limit_pages = 100\n",
    "\n",
    "term_search = ['politics','sports','cars','music','college']\n",
    "\n",
    "for i in term_search:\n",
    "    wik_sims(i, limit_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
